{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yelp Reviews Rating Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bangwei Zhou <br>\n",
    "James Jungsuk Lee <br>\n",
    "Ujjwal Peshin <br>\n",
    "Zhongling Jiang <br>\n",
    "\n",
    "## A. Abstract\n",
    "\n",
    "We compare the merits of n different recommendation algorithms side by side.\n",
    "\n",
    "* Biased Baseline\n",
    "* Collaborative Filtering (CF - Baseline)\n",
    "* Collective Matrix Factorization (CMF)\n",
    "* Content-Based Filtering (CBF)\n",
    "* Field-aware Factorization Machine (FFM) \n",
    "* Deep Learning Model\n",
    "\n",
    "* Hybrid Approach <br>\n",
    "\n",
    "Comparing the results of the models against our baseline shows that Collective Matrix Factorization Algorithm, DeepFM Algorithm, and Wide and Deep Algorithms outperform the baseline model. The strength of deep learning approaches were showcased as we increased the size of the dataset, beating the baselines initially by .02 RMSE in 20% subsampled data but later beating them by .04 RMSE in 100% of the data. Another key advantage of the deep learning approaches were the short training time. Using 100% of the dataset, they finished training in just 800 seconds on local environment while the CMF took an equivalent amount of time for dataset subsampled down to 50% size. Additionally, the weaknesses of the baseline models were exposed when metrics that measure user experiences such as ranking, coverage, and novelty were investigated. \n",
    "While our predictive algorithms didn’t provide drastic improvements in accuracy, our top deep learning methods providing about 3% improvement versus the baseline, given the fast training time as well as its ability to achieve superior measures on metrics that affect positive user experiences such as coverage, ranking, and novelty metrics, there is a strong reason to believe that deep learning models can provide value to Yelp users in a meaningful manner. In this report, we discuss the merits of various algorithms we explored and finally propose a complete recommendation system, that employs an ensemble of data retrieval system based on user location, cold-start recommendations using the bias-based model as well as DeepFM for general recommendation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Business Case and Objective\n",
    "\n",
    "Our goal through this exercise is to figure out the best methodology for recommending restaurants to our users. This will be largely measured by using RMSE, which measures our forecast’s error relative to the ground truth as well as ranking metrics. Additionally, we want to measure metrics that can help us assess the sanity of our recommendation and experiences of the users. We measure these using coverage and novelty metrics to ensure that there is enough diversity in our recommendations and that our model isn’t resorting to recommending the most popular restaurants as opposed to creating a truly personalized experience. \n",
    " \n",
    "There are additional considerations such as model computation time, which affects long term overhead of maintenance of the product, and model comprehension. These are important considerations for the business as inefficient solutions will add technology debt and burden to the company’s systems. Also, employing models that are well understood can help the team in getting buy-in from stakeholders. Therefore, these factors will be critically analyzed when assessing the merits of our algorithms. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Data\n",
    "\n",
    "We use the publicly available yelp dataset which consists roughly of 7 million reviews from 1.6 million users across 190 thousand businesses in 10 metropolitan areas. We are additionally provided with business and user metadata as well as check-in information, tips and photos provided by the reviewers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tarfile\n",
    "from tqdm import tqdm, tqdm_notebook, tnrange\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the decompress step takes a while and large disk space.\n",
    "# Run it only once\n",
    "zf = tarfile.open('yelp_dataset.tar') \n",
    "zf.extract('review.json')\n",
    "zf.extract('business.json')\n",
    "zf.extract('user.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6685900/6685900 [01:15<00:00, 88385.36it/s] \n"
     ]
    }
   ],
   "source": [
    "line_count = len(open(\"review.json\").readlines())\n",
    "user_ids, business_ids, stars, dates, texts = [], [], [], [], []\n",
    "with open(\"review.json\") as f:\n",
    "    for line in tqdm(f, total=line_count):\n",
    "        blob = json.loads(line)\n",
    "        user_ids += [blob[\"user_id\"]]\n",
    "        business_ids += [blob[\"business_id\"]]\n",
    "        stars += [blob[\"stars\"]]\n",
    "        dates += [blob[\"date\"]]\n",
    "        texts += [blob[\"text\"]]\n",
    "ratings_ = pd.DataFrame(\n",
    "    {\"user_id\": user_ids, \"business_id\": business_ids, \"rating\": stars, \"date\": dates, \"text\": texts}\n",
    ")\n",
    "user_counts = ratings_[\"user_id\"].value_counts()\n",
    "active_users = user_counts.loc[user_counts >= 5].index.tolist()\n",
    "ratings_ = ratings_.loc[ratings_.user_id.isin(active_users)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hG7b0MtEbXx5QzbzE6C_VA</td>\n",
       "      <td>ujmEBvifdJM6h6RLv4wQIg</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2013-05-07 04:34:36</td>\n",
       "      <td>Total bill for this horrible service? Over $8G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>n6-Gk65cPZL6Uz8qRm3NYw</td>\n",
       "      <td>WTqjgwHlXbSFevF32_DJVw</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2016-11-09 20:09:03</td>\n",
       "      <td>I have to say that this office really has it t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>jlu4CztcSxrKx56ba1a5AQ</td>\n",
       "      <td>3fw2X5bZYeW9xCz_zGhOHg</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2016-05-07 01:21:02</td>\n",
       "      <td>Tracy dessert had a big name in Hong Kong and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>d6xvYpyzcfbF_AZ8vMB7QA</td>\n",
       "      <td>zvO-PJCpNk4fgAVUnExYAA</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010-10-05 19:12:35</td>\n",
       "      <td>This place has gone down hill.  Clearly they h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sG_h0dIzTKWa3Q6fmb4u-g</td>\n",
       "      <td>b2jN2mm9Wf3RcrZCgfo1cg</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2015-01-18 14:04:18</td>\n",
       "      <td>I was really looking forward to visiting after...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  user_id             business_id  rating  \\\n",
       "0  hG7b0MtEbXx5QzbzE6C_VA  ujmEBvifdJM6h6RLv4wQIg     1.0   \n",
       "2  n6-Gk65cPZL6Uz8qRm3NYw  WTqjgwHlXbSFevF32_DJVw     5.0   \n",
       "6  jlu4CztcSxrKx56ba1a5AQ  3fw2X5bZYeW9xCz_zGhOHg     3.0   \n",
       "7  d6xvYpyzcfbF_AZ8vMB7QA  zvO-PJCpNk4fgAVUnExYAA     1.0   \n",
       "8  sG_h0dIzTKWa3Q6fmb4u-g  b2jN2mm9Wf3RcrZCgfo1cg     2.0   \n",
       "\n",
       "                  date                                               text  \n",
       "0  2013-05-07 04:34:36  Total bill for this horrible service? Over $8G...  \n",
       "2  2016-11-09 20:09:03  I have to say that this office really has it t...  \n",
       "6  2016-05-07 01:21:02  Tracy dessert had a big name in Hong Kong and ...  \n",
       "7  2010-10-05 19:12:35  This place has gone down hill.  Clearly they h...  \n",
       "8  2015-01-18 14:04:18  I was really looking forward to visiting after...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Users: 286130, unique restaurants: 185723\n"
     ]
    }
   ],
   "source": [
    "n_users = len(ratings_.user_id.unique())\n",
    "n_restaurants = len(ratings_.business_id.unique())\n",
    "print('Unique Users: {0}, unique restaurants: {1}'.format(n_users, n_restaurants))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in User attributes set and Restaurant attributes set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1637138/1637138 [00:32<00:00, 49990.93it/s]\n"
     ]
    }
   ],
   "source": [
    "# load user.json\n",
    "line_count = len(open(\"user.json\").readlines())\n",
    "users, names, review_counts, since, friends, useful, \\\n",
    "            funny, cool, n_fans, years_elite, average_stars = [], [], [], [], [], [], [], [], [], [], []\n",
    "with open(\"user.json\") as f:\n",
    "    for line in tqdm(f, total=line_count):\n",
    "        blob = json.loads(line)\n",
    "        users += [blob[\"user_id\"]]\n",
    "        names += [blob[\"name\"]]\n",
    "        review_counts += [blob[\"review_count\"]]\n",
    "        since += [blob[\"yelping_since\"]]\n",
    "        friends += [blob[\"friends\"]]\n",
    "        useful += [blob[\"useful\"]]\n",
    "        funny += [blob[\"funny\"]]\n",
    "        cool += [blob[\"cool\"]]\n",
    "        n_fans += [blob[\"fans\"]]\n",
    "        years_elite += [blob[\"elite\"]]\n",
    "        average_stars += [blob[\"average_stars\"]]\n",
    "        \n",
    "users_ = pd.DataFrame(\n",
    "    {\"user_id\": users, \n",
    "     \"user_name\": names,\n",
    "     \"user_review_count\": review_counts,\n",
    "     \"user_yelp_since\": since,\n",
    "     \"friends\": friends,\n",
    "     \"useful_reviews\": useful,\n",
    "     \"funny_reviews\": funny,\n",
    "     \"cool_reviews\": cool,\n",
    "     \"n_fans\": n_fans,\n",
    "     \"years_elite\": years_elite,\n",
    "     \"average_stars\": average_stars\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 192609/192609 [00:03<00:00, 62578.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# load business.json\n",
    "line_count = len(open(\"business.json\").readlines())\n",
    "business_ids, names, addresses, cities, states, latitudes, longitudes, stars, \\\n",
    "        review_counts, is_open, categories = [], [], [], [], [], [], [], [], [], [], []\n",
    "with open(\"business.json\") as f:\n",
    "    for line in tqdm(f, total=line_count):\n",
    "        blob = json.loads(line)\n",
    "        business_ids += [blob[\"business_id\"]]\n",
    "        names += [blob[\"name\"]]\n",
    "        addresses += [blob[\"address\"]]\n",
    "        cities += [blob[\"city\"]]\n",
    "        states += [blob[\"state\"]]\n",
    "        latitudes += [blob[\"latitude\"]]\n",
    "        longitudes += [blob[\"longitude\"]]\n",
    "        stars += [blob[\"stars\"]]\n",
    "        review_counts += [blob[\"review_count\"]]\n",
    "        is_open += [blob[\"is_open\"]]\n",
    "        categories += [blob[\"categories\"]]\n",
    "        \n",
    "business_ = pd.DataFrame(\n",
    "    {\"business_id\": business_ids, \n",
    "     \"business_name\": names,\n",
    "     \"business_address\": addresses,\n",
    "     \"business_city\": cities, \n",
    "     \"business_state\": states, \n",
    "     \"business_latitude\": latitudes, \n",
    "     \"business_longitude\": longitudes, \n",
    "     \"stars\": stars, \n",
    "     \"review_counts\": review_counts, \n",
    "     \"is_open\": is_open,\n",
    "     \"categories\": categories}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_ = users_.loc[users_.user_id.isin(active_users)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_name</th>\n",
       "      <th>user_review_count</th>\n",
       "      <th>user_yelp_since</th>\n",
       "      <th>friends</th>\n",
       "      <th>useful_reviews</th>\n",
       "      <th>funny_reviews</th>\n",
       "      <th>cool_reviews</th>\n",
       "      <th>n_fans</th>\n",
       "      <th>years_elite</th>\n",
       "      <th>average_stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>l6BmjZMeQD3rDxWUbiAiow</td>\n",
       "      <td>Rashmi</td>\n",
       "      <td>95</td>\n",
       "      <td>2013-10-08 23:11:33</td>\n",
       "      <td>c78V-rj8NQcQjOI8KP3UEA, alRMgPcngYSCJ5naFRBz5g...</td>\n",
       "      <td>84</td>\n",
       "      <td>17</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>2015,2016,2017</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4XChL029mKr5hydo79Ljxg</td>\n",
       "      <td>Jenna</td>\n",
       "      <td>33</td>\n",
       "      <td>2013-02-21 22:29:06</td>\n",
       "      <td>kEBTgDvFX754S68FllfCaA, aB2DynOxNOJK9st2ZeGTPg...</td>\n",
       "      <td>48</td>\n",
       "      <td>22</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td>3.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bc8C_eETBWL0olvFSJJd0w</td>\n",
       "      <td>David</td>\n",
       "      <td>16</td>\n",
       "      <td>2013-10-04 00:16:10</td>\n",
       "      <td>4N-HU_T32hLENLntsNKNBg, pSY2vwWLgWfGVAAiKQzMng...</td>\n",
       "      <td>28</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>3.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MM4RJAeH6yuaN8oZDSt0RA</td>\n",
       "      <td>Nancy</td>\n",
       "      <td>361</td>\n",
       "      <td>2013-10-23 07:02:50</td>\n",
       "      <td>mbwrZ-RS76V1HoJ0bF_Geg, g64lOV39xSLRZO0aQQ6DeQ...</td>\n",
       "      <td>1114</td>\n",
       "      <td>279</td>\n",
       "      <td>665</td>\n",
       "      <td>39</td>\n",
       "      <td>2015,2016,2017,2018</td>\n",
       "      <td>4.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TEtzbpgA2BFBrC0y0sCbfw</td>\n",
       "      <td>Keane</td>\n",
       "      <td>1122</td>\n",
       "      <td>2006-02-15 18:29:35</td>\n",
       "      <td>RJQTcJVlBsJ3_Yo0JSFQQg, GWt_h78k1CBBkE1NpThGfQ...</td>\n",
       "      <td>13311</td>\n",
       "      <td>19356</td>\n",
       "      <td>15319</td>\n",
       "      <td>696</td>\n",
       "      <td>2006,2007,2008,2009,2010,2011,2012,2013</td>\n",
       "      <td>4.39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  user_id user_name  user_review_count      user_yelp_since  \\\n",
       "0  l6BmjZMeQD3rDxWUbiAiow    Rashmi                 95  2013-10-08 23:11:33   \n",
       "1  4XChL029mKr5hydo79Ljxg     Jenna                 33  2013-02-21 22:29:06   \n",
       "2  bc8C_eETBWL0olvFSJJd0w     David                 16  2013-10-04 00:16:10   \n",
       "4  MM4RJAeH6yuaN8oZDSt0RA     Nancy                361  2013-10-23 07:02:50   \n",
       "6  TEtzbpgA2BFBrC0y0sCbfw     Keane               1122  2006-02-15 18:29:35   \n",
       "\n",
       "                                             friends  useful_reviews  \\\n",
       "0  c78V-rj8NQcQjOI8KP3UEA, alRMgPcngYSCJ5naFRBz5g...              84   \n",
       "1  kEBTgDvFX754S68FllfCaA, aB2DynOxNOJK9st2ZeGTPg...              48   \n",
       "2  4N-HU_T32hLENLntsNKNBg, pSY2vwWLgWfGVAAiKQzMng...              28   \n",
       "4  mbwrZ-RS76V1HoJ0bF_Geg, g64lOV39xSLRZO0aQQ6DeQ...            1114   \n",
       "6  RJQTcJVlBsJ3_Yo0JSFQQg, GWt_h78k1CBBkE1NpThGfQ...           13311   \n",
       "\n",
       "   funny_reviews  cool_reviews  n_fans  \\\n",
       "0             17            25       5   \n",
       "1             22            16       4   \n",
       "2              8            10       0   \n",
       "4            279           665      39   \n",
       "6          19356         15319     696   \n",
       "\n",
       "                               years_elite  average_stars  \n",
       "0                           2015,2016,2017           4.03  \n",
       "1                                                    3.63  \n",
       "2                                                    3.71  \n",
       "4                      2015,2016,2017,2018           4.08  \n",
       "6  2006,2007,2008,2009,2010,2011,2012,2013           4.39  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>business_name</th>\n",
       "      <th>business_address</th>\n",
       "      <th>business_city</th>\n",
       "      <th>business_state</th>\n",
       "      <th>business_latitude</th>\n",
       "      <th>business_longitude</th>\n",
       "      <th>stars</th>\n",
       "      <th>review_counts</th>\n",
       "      <th>is_open</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1SWheh84yJXfytovILXOAQ</td>\n",
       "      <td>Arizona Biltmore Golf Club</td>\n",
       "      <td>2818 E Camino Acequia Drive</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>AZ</td>\n",
       "      <td>33.522143</td>\n",
       "      <td>-112.018481</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>Golf, Active Life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>QXAEGFB4oINsVuTFxEYKFQ</td>\n",
       "      <td>Emerald Chinese Restaurant</td>\n",
       "      <td>30 Eglinton Avenue W</td>\n",
       "      <td>Mississauga</td>\n",
       "      <td>ON</td>\n",
       "      <td>43.605499</td>\n",
       "      <td>-79.652289</td>\n",
       "      <td>2.5</td>\n",
       "      <td>128</td>\n",
       "      <td>1</td>\n",
       "      <td>Specialty Food, Restaurants, Dim Sum, Imported...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gnKjwL_1w79qoiV3IC_xQQ</td>\n",
       "      <td>Musashi Japanese Restaurant</td>\n",
       "      <td>10110 Johnston Rd, Ste 15</td>\n",
       "      <td>Charlotte</td>\n",
       "      <td>NC</td>\n",
       "      <td>35.092564</td>\n",
       "      <td>-80.859132</td>\n",
       "      <td>4.0</td>\n",
       "      <td>170</td>\n",
       "      <td>1</td>\n",
       "      <td>Sushi Bars, Restaurants, Japanese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>xvX2CttrVhyG2z1dFg_0xw</td>\n",
       "      <td>Farmers Insurance - Paul Lorenz</td>\n",
       "      <td>15655 W Roosevelt St, Ste 237</td>\n",
       "      <td>Goodyear</td>\n",
       "      <td>AZ</td>\n",
       "      <td>33.455613</td>\n",
       "      <td>-112.395596</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Insurance, Financial Services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HhyxOkGAM07SRYtlQ4wMFQ</td>\n",
       "      <td>Queen City Plumbing</td>\n",
       "      <td>4209 Stuart Andrew Blvd, Ste F</td>\n",
       "      <td>Charlotte</td>\n",
       "      <td>NC</td>\n",
       "      <td>35.190012</td>\n",
       "      <td>-80.887223</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Plumbing, Shopping, Local Services, Home Servi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id                    business_name  \\\n",
       "0  1SWheh84yJXfytovILXOAQ       Arizona Biltmore Golf Club   \n",
       "1  QXAEGFB4oINsVuTFxEYKFQ       Emerald Chinese Restaurant   \n",
       "2  gnKjwL_1w79qoiV3IC_xQQ      Musashi Japanese Restaurant   \n",
       "3  xvX2CttrVhyG2z1dFg_0xw  Farmers Insurance - Paul Lorenz   \n",
       "4  HhyxOkGAM07SRYtlQ4wMFQ              Queen City Plumbing   \n",
       "\n",
       "                 business_address business_city business_state  \\\n",
       "0     2818 E Camino Acequia Drive       Phoenix             AZ   \n",
       "1            30 Eglinton Avenue W   Mississauga             ON   \n",
       "2       10110 Johnston Rd, Ste 15     Charlotte             NC   \n",
       "3   15655 W Roosevelt St, Ste 237      Goodyear             AZ   \n",
       "4  4209 Stuart Andrew Blvd, Ste F     Charlotte             NC   \n",
       "\n",
       "   business_latitude  business_longitude  stars  review_counts  is_open  \\\n",
       "0          33.522143         -112.018481    3.0              5        0   \n",
       "1          43.605499          -79.652289    2.5            128        1   \n",
       "2          35.092564          -80.859132    4.0            170        1   \n",
       "3          33.455613         -112.395596    5.0              3        1   \n",
       "4          35.190012          -80.887223    4.0              4        1   \n",
       "\n",
       "                                          categories  \n",
       "0                                  Golf, Active Life  \n",
       "1  Specialty Food, Restaurants, Dim Sum, Imported...  \n",
       "2                  Sushi Bars, Restaurants, Japanese  \n",
       "3                      Insurance, Financial Services  \n",
       "4  Plumbing, Shopping, Local Services, Home Servi...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "business_.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E. Subsampling and Train-Test Split\n",
    "#### E.1. FIltering, Undersampling and Dataset Size Considerations\n",
    "The idea behind undersampling is to develop a smaller dataset that is representable, or partially representative of the entire dataset, and expedites the model development cycle. We utilize 20% dataset size generally across the board for fast development iteration and for computation intensive tasks such as hyperparameter tuning. However, it is critical to analyze model performance across dataset sizes since the models’ capability to handle large dataset is an important consideration if it were to be utilized in production. Therefore we investigate two other dataset sizes as well, 50% and 100% of the data size. Whereas 20% of the dataset size is used generally, larger datasets are employed to analyze certain models’ capability to handle large data as well as measure how their performance changes with larger datasets. Finally, we filter our dataset to only active users who have at least 5 reviews since there needs to be at least some data about the user for the model to perform. If a user does not have at least 5 reviews, we will be building out recommendations using cold-start methods.\n",
    "\n",
    "#### E.2 Train Test Split\n",
    "\n",
    "Train-Test split is done in a very straightforward way. We take all the users that made through our filters as described above. Then we simply take the last two reviews of the users. The most recent review becomes our test set and the other second most recent review becomes our validation set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLING_RATE = 1/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  user_id             business_id  rating  \\\n",
      "0  n6-Gk65cPZL6Uz8qRm3NYw  WTqjgwHlXbSFevF32_DJVw     5.0   \n",
      "1  n6-Gk65cPZL6Uz8qRm3NYw  hk5wpV-_pi5jmDDVPeG8DA     5.0   \n",
      "2  n6-Gk65cPZL6Uz8qRm3NYw  30Q5xBagQHmkwp8Q9I1FCg     5.0   \n",
      "3  n6-Gk65cPZL6Uz8qRm3NYw  UtWngqS-WloIY_A53W5K-Q     5.0   \n",
      "4  n6-Gk65cPZL6Uz8qRm3NYw  dU-Nt1-LjV9mAgFOVcdAJw     5.0   \n",
      "\n",
      "                  date                                               text  \n",
      "0  2016-11-09 20:09:03  I have to say that this office really has it t...  \n",
      "1  2018-09-14 18:50:19  I highly recommend Arizona Pet Mortuary, David...  \n",
      "2  2018-02-03 23:27:43  First time at this restaurant our server \"Ramo...  \n",
      "3  2016-02-18 06:42:16  Such an amazing hospital with friendly staff, ...  \n",
      "4  2018-08-15 22:14:18  Went for my yearly GYN exam and was seen by Lo...  \n",
      "(918368, 5)\n"
     ]
    }
   ],
   "source": [
    "# Downsample by users\n",
    "user_id_unique = ratings_.user_id.unique()\n",
    "user_id_sample = pd.DataFrame(user_id_unique, columns=['unique_user_id']) \\\n",
    "                    .sample(frac= SAMPLING_RATE, replace=False, random_state=1)\n",
    "\n",
    "ratings_sample = ratings_.merge(user_id_sample, left_on='user_id', right_on='unique_user_id') \\\n",
    "                    .drop(['unique_user_id'], axis=1)\n",
    "print(ratings_sample.head())\n",
    "print(ratings_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 803897 rows, 5 columns in training set.\n",
      "There are 57229 rows, 5 columns in training set.\n",
      "There are 57223 rows, 5 columns in holdout set.\n"
     ]
    }
   ],
   "source": [
    "# hold out last review\n",
    "ratings_user_date = ratings_sample.loc[:, ['user_id', 'date']]\n",
    "ratings_user_date.date = pd.to_datetime(ratings_user_date.date)\n",
    "index_holdout = ratings_user_date.groupby(['user_id'], sort=False)['date'].transform(max) == ratings_user_date['date']\n",
    "ratings_holdout_ = ratings_sample[index_holdout]\n",
    "ratings_traincv_ = ratings_sample[~index_holdout]\n",
    "\n",
    "ratings_user_date = ratings_traincv_.loc[:, ['user_id', 'date']]\n",
    "index_holdout = ratings_user_date.groupby(['user_id'], sort=False)['date'].transform(max) == ratings_user_date['date']\n",
    "ratings_cv_ = ratings_traincv_[index_holdout]\n",
    "ratings_train_ = ratings_traincv_[~index_holdout]\n",
    "\n",
    "# remove the user that has fake reviews \n",
    "\n",
    "cv_users_del = set(ratings_cv_.user_id) - set(ratings_train_.user_id)\n",
    "holdout_users_del = set(ratings_holdout_.user_id) - set(ratings_train_.user_id)\n",
    "ratings_cv_ = ratings_cv_[~ratings_cv_.user_id.isin(cv_users_del)]\n",
    "ratings_holdout_ = ratings_holdout_[~ratings_holdout_.user_id.isin(holdout_users_del)]\n",
    "\n",
    "# ratings_cv_ = ratings_cv_[~ratings_cv_.user_id.isin(['HiT9sg9pvDiEVMFHJYihXg'])]\n",
    "# ratings_holdout_ = ratings_holdout_[~ratings_holdout_.user_id.isin(['HiT9sg9pvDiEVMFHJYihXg'])]\n",
    "\n",
    "print('There are {0} rows, {1} columns in training set.'.format(ratings_train_.shape[0], ratings_train_.shape[1]))\n",
    "print('There are {0} rows, {1} columns in training set.'.format(ratings_cv_.shape[0], ratings_cv_.shape[1]))\n",
    "print('There are {0} rows, {1} columns in holdout set.'.format(ratings_holdout_.shape[0], ratings_holdout_.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57223\n"
     ]
    }
   ],
   "source": [
    "# check if we have a enough user sample size (> 50000)\n",
    "number_of_unique_users = len(ratings_train_.user_id.unique())\n",
    "print(number_of_unique_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F. Evaluation Metrics\n",
    "\n",
    "A list of evaluation metrics the team uses are:\n",
    "\n",
    "#### F.1. Regression Metrics\n",
    "Regression metrics measure We present four different regression metrics that are measured but we primarily use the RMSE. \n",
    "Root Mean Square Error (RMSE) calculates square rooted sum of square residuals of predictions. It measures numerical difference between all ground-truth ratings and actual ratings in test set.\n",
    "Mean Absolute Error (MAE) calculates sum of absolute residuals. It measures numerical difference between all ground-truth ratings and actual ratings in test set, and more robust to outliers.\n",
    "R-squared measures what percentage of variance in target that is explained by predictions. The higher the value, the better the predictions.\n",
    "\n",
    "#### F.2. Ranking Metrics\n",
    "We first make top 10 recommendations of restaurants for each user and see how relevant the rankings were. First we discuss the metrics then discuss a few caveats to the approach we took to measure them.\n",
    "Inclusion of Last Review in Top 10 Recommendations: We train our model on the training set and make top 10 recommendations to the users. We then look at the test set and see if the user’s latest review was included in that top 10 recommendations. We calculate the proportion of users who received such recommendations. In other words we have,\n",
    "\n",
    "$$\\frac{1}{n} * \\sum_j^n \\sum_{i \\in j's top 10}^{10}Rel(i)$$\n",
    "\n",
    "where a recommendation is relevant if it is the latest visited restaurant of the user and n is the total number of users measured. \n",
    "Average Ranking of Latest Restaurant: We train our model on the training set and make a prediction on every single business  and user combination. We then measure the average rankings on that prediction of the latest business that the user visited. In other words we have,\n",
    "\n",
    "$$\\frac{1}{n} * \\sum_j^n\\sum_i^m Rank(i)(1_{i=latest business of j})$$\n",
    "\n",
    "Where the indicator function is one if the restaurant is the last visited restaurant of user j, n is the total number of users and m is the total number of businesses.\n",
    "Some caveats of our approach is that instead of making a prediction on the entire business universe, we make predictions on the businesses that are in the same city as the business of the user’s last review. This makes sense since it would be futile to recommend a restaurant in Los Angeles to a user who is in New York City. This also allows us to reduce the computation time of our recommendation, which is another key advantage we want to have when we serve our model to the users. Additionally, we measure the above two metrics on a subsample of users as opposed to all the users to save computation time, and we also only take subsample of ratings that were positive ratings since we want to measure how good our recommendations are. \n",
    "#### F.3. Coverage\n",
    "We measure the coverage of our recommendations by looking at the proportion of our recommendations that are distinct. In other words, we measure, <br>\n",
    "\n",
    "$$\\frac{number of distinct businesses in all recommendations to the subsample} \n",
    "{number of all recommendations to the subsample}$$\n",
    "\n",
    " \n",
    "#### F.4. Novelty\n",
    "We measure the novelty of our recommendations by simply taking the proportion of businesses in our top ten recommendations that the user hasn’t been to. This is crucial since we don’t want our recommendations to be filled with restaurants that the user has already been to. We measure novelty as simply. \n",
    "\n",
    "#### F.5. Runtime \n",
    "We measure the runtime of the model’s training as well as its prediction time on validation set using Python’s time library. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics on Coverage and Serendipity\n",
    "\n",
    "First subsample a group of users that we will measure these metrics from\n",
    "\n",
    "Methodology:\n",
    "    We sample 5 users from each city where the user made the latest review.\n",
    "    These cities must have at least 100 unique businesses\n",
    "    These users must also have made a postive review(above their historical average)to those restaurants.\n",
    "        1. We recommend 10 restaurants to each user\n",
    "        2. We see if their latest restaurant makes it into the top 10 list (Ranking Metric)\n",
    "        3. We see for those 10 x 5 recommendations, how many of them are distinct businesses (Coverage)\n",
    "        4. We see for those top 10 recommendations, how many of them are restaurants they have not visited (Serendipity)\n",
    "    \n",
    "    Additionally, we measure what our ranking was for the latest restaurant that the user visited(Ranking Metric\n",
    "    \n",
    "\n",
    "\n",
    "Criteria: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(df):\n",
    "    df['date']  = pd.to_datetime(df['date'])\n",
    "    df['week_day'] = df['date'].dt.weekday\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['hour'] = df['date'].dt.hour\n",
    "    df = df.merge(users_, on = 'user_id')\n",
    "    df = df.merge(business_, on = 'business_id')\n",
    "    rename_dict = {'business_longitude': 'longitude', 'business_latitude': 'latitude',\n",
    "                  'business_state':'state','business_city':'city', 'business_address': 'address'}\n",
    "    df = df.rename(columns = rename_dict)\n",
    "    return df\n",
    "\n",
    "ratings_train = process(ratings_train_.copy())\n",
    "ratings_holdout = process(ratings_holdout_.copy())\n",
    "ratings_val = process(ratings_cv_.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_train_final = ratings_train.append(ratings_val)\n",
    "ratings_entire_df = ratings_train.append(ratings_val).append(ratings_holdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratings_holdout.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_city_businesses = ratings_entire_df[['city','business_id']].drop_duplicates()\n",
    "unique_cities = unique_city_businesses.groupby('city').count()['business_id']\n",
    "unique_cities = unique_cities[unique_cities > 100]\n",
    "out = pd.DataFrame()\n",
    "for city in unique_cities.index:\n",
    "    tmp = ratings_holdout[(ratings_holdout['city'] ==city) &\n",
    "                              (ratings_holdout['rating'] >ratings_holdout['average_stars'])]\n",
    "    if len(tmp['user_id'].unique())>4:\n",
    "        \n",
    "        ###this weird sampling technique is to ensure we dont' sample the same user twice in a same city\n",
    "        five_users = np.random.choice(tmp['user_id'].unique(),5, replace = False)\n",
    "        row = tmp[tmp['user_id'].isin(five_users)].groupby('user_id', group_keys=False).apply(lambda df: df.sample(1))\n",
    "        out = out.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_df = out[['user_id','city','state']]\n",
    "predict_df = predict_df.merge(unique_city_businesses, on = 'city')\n",
    "predict_df.to_csv('data/metric_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random initialization, needs to be substituted by actual predictions later.\n",
    "predict_df['predictions'] = 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_metrics(predict_df, validation_subsample, ratings_train_final):\n",
    "    top_10_recs = predict_df.groupby(['user_id','city'])['predictions'].nlargest(10).reset_index()\n",
    "    out = validation_subsample\n",
    "    cnt =0\n",
    "    serendipity = 0\n",
    "    \n",
    "    \n",
    "    for row in out.iterrows():\n",
    "        row_values = row[1]\n",
    "        top_10 = predict_df.loc[top_10_recs[top_10_recs['user_id'] == row_values['user_id']].level_2]['business_id']\n",
    "        ###In top 10\n",
    "        if row_values['business_id'] in top_10.values:\n",
    "            cnt+=1\n",
    "        user_history = ratings_train_final[ratings_train_final['user_id'] == row_values['user_id']]    \n",
    "        been_there = [i for i in top_10.values if i in  user_history.business_id.values]\n",
    "        serendipity += 1-len(been_there)/10\n",
    "    \n",
    "    top_10 = cnt/len(out)\n",
    "    serendipity = serendipity/len(out)\n",
    "    \n",
    "    predict_df = predict_df.reset_index()\n",
    "    \n",
    "    analysis_df = predict_df.merge(top_10_recs, left_on = ['user_id','city','index'], \\\n",
    "                                   right_on = ['user_id','city','level_2'])\n",
    "    \n",
    "    coverage = (analysis_df.groupby('city')['business_id'].nunique()/50).values.mean()\n",
    "    \n",
    "    predict_df['rankings']=predict_df.groupby(['city','user_id'])['predictions']. \\\n",
    "                                                        rank(method=\"first\",ascending = False)\n",
    "    running_rankings =0\n",
    "    for row in out.iterrows():\n",
    "        row_values = row[1]\n",
    "        user_recs = predict_df[(predict_df['user_id']==row_values['user_id'])\n",
    "                            &(predict_df['city']==row_values['city'])\n",
    "                             & (predict_df['business_id']==row_values['business_id'])\n",
    "                              ]\n",
    "        assert len(user_recs)==1\n",
    "        running_rankings += user_recs['rankings'].sum()\n",
    "\n",
    "    avg_rank = running_rankings / len(out)\n",
    "    print(top_10, coverage, serendipity, avg_rank)\n",
    "    \n",
    "    return top_10, coverage, serendipity, avg_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F. Methods\n",
    "\n",
    "A list of models that the team attempts are: \n",
    "\n",
    "* Bias Baseline\n",
    "* Collaborative Filtering Baseline: SVD\n",
    "* Collective Matrix Factorization (CMF)\n",
    "* Content-Based Filtering (CBF)\n",
    "* Field-aware Factorization Machine (FFM) \n",
    "* Deep Learning Model\n",
    "\n",
    "_**Note**_: the team has run algorithms on **20%, 50%, 100%** training data respectively. But for readability purpose, only result on 20% data is displayed in this notebook.\n",
    "\n",
    "### Baseline models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline 1: Bias Baseline\n",
    "\n",
    "$\\sum_{r_{ui} \\in R_{train}} \\left(r_{ui} - (\\mu + b_u + b_i)\\right)^2 +\n",
    "\\lambda \\left(b_u^2 + b_i^2 \\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-surprise\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/da/b5700d96495fb4f092be497f02492768a3d96a3f4fa2ae7dea46d4081cfa/scikit-surprise-1.1.0.tar.gz (6.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 6.5MB 6.2MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /Users/zhonglingjiang/anaconda3/lib/python3.6/site-packages (from scikit-surprise) (0.14.1)\n",
      "Requirement already satisfied: numpy>=1.11.2 in /Users/zhonglingjiang/anaconda3/lib/python3.6/site-packages (from scikit-surprise) (1.14.3)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /Users/zhonglingjiang/anaconda3/lib/python3.6/site-packages (from scikit-surprise) (1.1.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /Users/zhonglingjiang/.local/lib/python3.6/site-packages (from scikit-surprise) (1.13.0)\n",
      "Building wheels for collected packages: scikit-surprise\n",
      "  Running setup.py bdist_wheel for scikit-surprise ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/zhonglingjiang/Library/Caches/pip/wheels/cc/fa/8c/16c93fccce688ae1bde7d979ff102f7bee980d9cfeb8641bcf\n",
      "Successfully built scikit-surprise\n",
      "Installing collected packages: scikit-surprise\n",
      "Successfully installed scikit-surprise-1.1.0\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from surprise import SVD\n",
    "from surprise import accuracy\n",
    "from surprise import Reader\n",
    "from surprise.model_selection import GridSearchCV\n",
    "from surprise import Dataset\n",
    "from surprise import BaselineOnly\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(df):\n",
    "    df['date']  = pd.to_datetime(df['date'])\n",
    "    df['week_day'] = df['date'].dt.weekday\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['hour'] = df['date'].dt.hour\n",
    "    df = df.merge(users_, on = 'user_id')\n",
    "    df = df.merge(business_, on = 'business_id')\n",
    "    rename_dict = {'business_longitude': 'longitude', 'business_latitude': 'latitude',\n",
    "              'business_state':'state','business_city':'city', 'business_address': 'address'}\n",
    "    df = df.rename(columns = rename_dict)\n",
    "    return df\n",
    "\n",
    "ratings_train = process(ratings_train_.copy())\n",
    "ratings_test = process(ratings_holdout_.copy())\n",
    "ratings_val = process(ratings_cv_.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove observations that may cause cold-start problem, which breaks the model.\n",
    "ratings_test = ratings_test.loc[ratings_test.business_id.isin(ratings_train.business_id)]\n",
    "ratings_val = ratings_val.loc[ratings_val.business_id.isin(ratings_train.business_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = ratings_train.loc[:,['user_id', 'business_id', 'rating']]\n",
    "trainset.columns = ['userID', 'itemID','rating']\n",
    "valset = ratings_val.loc[:, ['user_id', 'business_id', 'rating']]\n",
    "valset.columns = ['userID', 'itemID','rating']\n",
    "testset = ratings_holdout.loc[:, ['user_id', 'business_id', 'rating']]\n",
    "testset.columns = ['userID', 'itemID','rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale = (0.0, 5.0))\n",
    "train_data = Dataset.load_from_df(trainset[['userID','itemID','rating']], reader)\n",
    "val_data = Dataset.load_from_df(valset[['userID','itemID','rating']], reader)\n",
    "test_data = Dataset.load_from_df(testset[['userID','itemID','rating']], reader)\n",
    "\n",
    "train_sr = train_data.build_full_trainset()\n",
    "val_sr_before = val_data.build_full_trainset()\n",
    "val_sr = val_sr_before.build_testset()\n",
    "test_sr_before = test_data.build_full_trainset()\n",
    "test_sr = test_sr_before.build_testset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "RMSE: 1.3322\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.3322167380674812"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bsl_options = {'method': 'als', 'n_epochs':3}\n",
    "bias_baseline = BaselineOnly(bsl_options)\n",
    "bias_baseline.fit(train_sr)\n",
    "predictions = bias_baseline.test(val_sr)\n",
    "accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "RMSE: 1.3323\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.332251167562718"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bsl_options = {'method': 'als', 'n_epochs':5}\n",
    "bias_baseline = BaselineOnly(bsl_options)\n",
    "bias_baseline.fit(train_sr)\n",
    "predictions = bias_baseline.test(val_sr)\n",
    "accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "RMSE: 1.3323\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.3322543575081625"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bsl_options = {'method': 'als', 'n_epochs':9}\n",
    "bias_baseline = BaselineOnly(bsl_options)\n",
    "bias_baseline.fit(train_sr)\n",
    "predictions = bias_baseline.test(val_sr)\n",
    "accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: It seems different hyperparameters all performs the same result; the team just uses default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "**Note**: the team has evaluated this and the following algorithms on **20%, 50%, 100%** training data respectively. For readability, only result on 20% data is displayed. For full result, please refer to a result table enclosed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "--- 2.340609073638916 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# runtime on 20% data\n",
    "start_time = time.time()\n",
    "bias_baseline.fit(train_sr)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.3811\n",
      "R^2 (with 20% data):  0.1661144204822098\n",
      "MAE (with 20% data):  1.1598083269638433\n",
      "--- 0.17791104316711426 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Regression metrics \n",
    "bbase_p = bias_baseline.test(test_sr)\n",
    "start_time = time.time()\n",
    "bbase_20_df = pd.DataFrame(bbase_p, columns = ['userId','itemId','rating','pred_rating','x'])\n",
    "accuracy.rmse(bbase_p)\n",
    "print('R^2 (with 20% data): ', r2_score(bbase_20_df.rating , bbase_20_df.pred_rating))\n",
    "print('MAE (with 20% data): ', mean_absolute_error(bbase_20_df.rating, bbase_20_df.pred_rating))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n"
     ]
    }
   ],
   "source": [
    "# Ranking + coverage + novelty metrics\n",
    "predict_df_baseline = predict_df.copy()\n",
    "eval_set = Dataset.load_from_df(predict_df_baseline[['user_id','business_id','predictions']], reader)\n",
    "\n",
    "bias_basline = BaselineOnly({'method': 'als', 'n_epochs':9})\n",
    "eval_before = eval_set.build_full_trainset()\n",
    "eval_sr = eval_before.build_testset()\n",
    "bias_basline.fit(train_sr)\n",
    "eval_pred = bias_basline.test(eval_sr)\n",
    "#accuracy.rmse(predictions_50)\n",
    "baseline_20 = pd.DataFrame(eval_pred, columns = ['userId','itemId','rating','pred_rating','x'])\n",
    "predict_df_baseline['predictions'] = baseline_20.pred_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12325581395348838 0.20441860465116274 0.9795348837209297 526.606976744186\n"
     ]
    }
   ],
   "source": [
    "top_10, coverage, serendipity, avg_rank = get_all_metrics(predict_df_baseline, out, ratings_train_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline 2: Collaborative Filtering via SVD\n",
    "\n",
    "Matrix factorization is a class of collaborative filtering algorithms. The general idea behind matrix factorization is that there can exist a lower dimensional latent space of features in which users and items can be represented such that the interaction between them can be obtained by simply dot producing the corresponding dense vectors in that space. In short, it decomposes a m*n user-item interaction matrix into two m*k and k*n matrices, sharing a joint latent vector space, where m represents the number of users, and n represents the number of items. In terms of its outcome, we are likely to observe that close users in terms of preferences as well as close items in terms of characteristics can have close representations in the latent space.\n",
    "\n",
    "The mathematical overview is as follows:\n",
    "Given a n*m matrix, such that . X is the user matrix where rows represent the n users and Y is the item matrix where rows represent the m items. We want to search for the dot product of matrices X and Y that best approximate the existing interactions; i.e., we want to find X and Y that minimize the “rating reconstruction error”:\n",
    "\n",
    "$$ (X,Y) = argmin_{X,Y} \\sum_{(i,j) \\in E} [(X_i)(Y_j)^T − M_{ij}]^2$$\n",
    "\n",
    "Adding a regularization term, we can also get:\n",
    "\n",
    "$$(X,Y) = argmin_{X,Y} ½ \\sum_{(i,j) \\in E} [(X_i)(Y_j)^T − M_{ij}]^2 + \\lambda/2(\\sum_{i,k}(X_{ik})^2 + \\sum_{j,k}(Y_{jk})^2)$$\n",
    "\n",
    "In general, we obtain the matrices X and Y following a gradient descent optimization process. And once the matrices are obtained, we can predict the ratings simply by multiplying the user vector by any item vector.\n",
    "\n",
    "In this Yelp Rating Challenge, we used the python surprise package to implement MF. The MF algorithm there uses the SVD approach, which is essentially \n",
    "\n",
    "$$ P_{m * n} = U_{m * m} \\sum_{m * n} V_{n * n}$$\n",
    "\n",
    "There, the prediction is\n",
    " $$\\hat(r_{ui}) = \\mu + b_u + b_i + (q_i)^T p_u $$\n",
    " \n",
    "and the regularized squared error that needs to be minimized is \n",
    "\n",
    "$$\\sum_{r_{ui} \\in R_{train}} (r_{ui} − \\hat(r_{ui}))^2 + \\lambda(b^2_{i} + b^2_{u} + ||q_i||^2 + ||p_u||^2)$$\n",
    "\n",
    "As the way the package is designed, we tuned on n_epochs, lr_all and leg_all to get an optimal hyperparameter set, where n_epochs is the number of iterations of the SGD (stochastic gradient descent) procedure, lr_all is the learning rate for all parameters, and reg_all is the regularization term for all parameters.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will follow the same process as the above model so can be skipped\n",
    "def process(df):\n",
    "    df['date']  = pd.to_datetime(df['date'])\n",
    "    df['week_day'] = df['date'].dt.weekday\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['hour'] = df['date'].dt.hour\n",
    "    df = df.merge(users_, on = 'user_id')\n",
    "    df = df.merge(business_, on = 'business_id')\n",
    "    rename_dict = {'business_longitude': 'longitude', 'business_latitude': 'latitude',\n",
    "              'business_state':'state','business_city':'city', 'business_address': 'address'}\n",
    "    df = df.rename(columns = rename_dict)\n",
    "    return df\n",
    "ratings_train = process(ratings_train_.copy())\n",
    "ratings_test = process(ratings_holdout_.copy())\n",
    "ratings_val = process(ratings_cv_.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_test = ratings_test.loc[ratings_test.business_id.isin(ratings_train.business_id)]\n",
    "ratings_val = ratings_val.loc[ratings_val.business_id.isin(ratings_train.business_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = ratings_train.loc[:,['user_id', 'business_id', 'rating']]\n",
    "trainset.columns = ['userID', 'itemID','rating']\n",
    "valset = ratings_val.loc[:, ['user_id', 'business_id', 'rating']]\n",
    "valset.columns = ['userID', 'itemID','rating']\n",
    "testset = ratings_holdout.loc[:, ['user_id', 'business_id', 'rating']]\n",
    "testset.columns = ['userID', 'itemID','rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale = (0.0, 5.0))\n",
    "train_data = Dataset.load_from_df(trainset[['userID','itemID','rating']], reader)\n",
    "val_data = Dataset.load_from_df(valset[['userID','itemID','rating']], reader)\n",
    "test_data = Dataset.load_from_df(testset[['userID','itemID','rating']], reader)\n",
    "\n",
    "train_sr = train_data.build_full_trainset()\n",
    "val_sr_before = val_data.build_full_trainset()\n",
    "val_sr = val_sr_before.build_testset()\n",
    "test_sr_before = test_data.build_full_trainset()\n",
    "test_sr = test_sr_before.build_testset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting n: 10, l: 0.001, r: 0.02\n",
      "RMSE: 1.4159\n",
      "Fitting n: 10, l: 0.001, r: 0.05\n",
      "RMSE: 1.4153\n",
      "Fitting n: 10, l: 0.001, r: 0.1\n",
      "RMSE: 1.4150\n",
      "Fitting n: 10, l: 0.001, r: 0.4\n",
      "RMSE: 1.4172\n",
      "Fitting n: 10, l: 0.001, r: 0.5\n",
      "RMSE: 1.4174\n",
      "Fitting n: 10, l: 0.003, r: 0.02\n",
      "RMSE: 1.3687\n",
      "Fitting n: 10, l: 0.003, r: 0.05\n",
      "RMSE: 1.3690\n",
      "Fitting n: 10, l: 0.003, r: 0.1\n",
      "RMSE: 1.3691\n",
      "Fitting n: 10, l: 0.003, r: 0.4\n",
      "RMSE: 1.3727\n",
      "Fitting n: 10, l: 0.003, r: 0.5\n",
      "RMSE: 1.3737\n",
      "Fitting n: 10, l: 0.005, r: 0.02\n",
      "RMSE: 1.3448\n",
      "Fitting n: 10, l: 0.005, r: 0.05\n",
      "RMSE: 1.3454\n",
      "Fitting n: 10, l: 0.005, r: 0.1\n",
      "RMSE: 1.3447\n",
      "Fitting n: 10, l: 0.005, r: 0.4\n",
      "RMSE: 1.3493\n",
      "Fitting n: 10, l: 0.005, r: 0.5\n",
      "RMSE: 1.3508\n",
      "Fitting n: 20, l: 0.001, r: 0.02\n",
      "RMSE: 1.3868\n",
      "Fitting n: 20, l: 0.001, r: 0.05\n",
      "RMSE: 1.3871\n",
      "Fitting n: 20, l: 0.001, r: 0.1\n",
      "RMSE: 1.3873\n",
      "Fitting n: 20, l: 0.001, r: 0.4\n",
      "RMSE: 1.3902\n",
      "Fitting n: 20, l: 0.001, r: 0.5\n",
      "RMSE: 1.3904\n",
      "Fitting n: 20, l: 0.003, r: 0.02\n",
      "RMSE: 1.3378\n",
      "Fitting n: 20, l: 0.003, r: 0.05\n",
      "RMSE: 1.3379\n",
      "Fitting n: 20, l: 0.003, r: 0.1\n",
      "RMSE: 1.3379\n",
      "Fitting n: 20, l: 0.003, r: 0.4\n",
      "RMSE: 1.3416\n",
      "Fitting n: 20, l: 0.003, r: 0.5\n",
      "RMSE: 1.3436\n",
      "Fitting n: 20, l: 0.005, r: 0.02\n",
      "RMSE: 1.3209\n",
      "Fitting n: 20, l: 0.005, r: 0.05\n",
      "RMSE: 1.3199\n",
      "Fitting n: 20, l: 0.005, r: 0.1\n",
      "RMSE: 1.3196\n",
      "Fitting n: 20, l: 0.005, r: 0.4\n",
      "RMSE: 1.3233\n",
      "Fitting n: 20, l: 0.005, r: 0.5\n",
      "RMSE: 1.3262\n",
      "Fitting n: 30, l: 0.001, r: 0.02\n",
      "RMSE: 1.3689\n",
      "Fitting n: 30, l: 0.001, r: 0.05\n",
      "RMSE: 1.3691\n",
      "Fitting n: 30, l: 0.001, r: 0.1\n",
      "RMSE: 1.3692\n",
      "Fitting n: 30, l: 0.001, r: 0.4\n",
      "RMSE: 1.3718\n",
      "Fitting n: 30, l: 0.001, r: 0.5\n",
      "RMSE: 1.3734\n",
      "Fitting n: 30, l: 0.003, r: 0.02\n",
      "RMSE: 1.3246\n",
      "Fitting n: 30, l: 0.003, r: 0.05\n",
      "RMSE: 1.3240\n",
      "Fitting n: 30, l: 0.003, r: 0.1\n",
      "RMSE: 1.3229\n",
      "Fitting n: 30, l: 0.003, r: 0.4\n",
      "RMSE: 1.3267\n",
      "Fitting n: 30, l: 0.003, r: 0.5\n",
      "RMSE: 1.3292\n",
      "Fitting n: 30, l: 0.005, r: 0.02\n",
      "RMSE: 1.3185\n",
      "Fitting n: 30, l: 0.005, r: 0.05\n",
      "RMSE: 1.3141\n",
      "Fitting n: 30, l: 0.005, r: 0.1\n",
      "RMSE: 1.3098\n",
      "Fitting n: 30, l: 0.005, r: 0.4\n",
      "RMSE: 1.3139\n",
      "Fitting n: 30, l: 0.005, r: 0.5\n",
      "RMSE: 1.3166\n"
     ]
    }
   ],
   "source": [
    "RMSE_tune = {}\n",
    "n_epochs = [10, 20, 30]  # the number of iteration of the SGD procedure\n",
    "lr_all = [0.001, 0.003, 0.005] # the learning rate for all parameters\n",
    "reg_all =  [0.02, 0.05, 0.1, 0.4, 0.5] # the regularization term for all parameters\n",
    "\n",
    "for n in n_epochs:\n",
    "    for l in lr_all:\n",
    "        for r in reg_all:\n",
    "            print('Fitting n: {0}, l: {1}, r: {2}'.format(n, l, r))\n",
    "            algo = SVD(n_epochs = n, lr_all = l, reg_all = r)\n",
    "            algo.fit(train_sr)\n",
    "            predictions = algo.test(val_sr)\n",
    "            RMSE_tune[n,l,r] = accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 0.005, 0.1)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator\n",
    "min(RMSE_tune.items(), key=operator.itemgetter(1))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: The best combination is when n_epochs = 30, lr_all = 0.005, reg_all = 0.1, and the RMSE score on validation set is 1.3098"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and test on the optimal parameter\n",
    "start_time = time.time()\n",
    "algo_real = SVD(n_epochs = 30, lr_all = 0.005, reg_all = 0.1)\n",
    "algo_real.fit(train_sr)\n",
    "predictions = algo_real.test(test_sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 83.35147786140442 seconds ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.3595\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.359467895040839"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  1.1170\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.1169825502216897"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.mae(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19207896312764172"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score([t[2] for t in predictions], [t[3] for t in predictions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To evaluate coverage and serendipity metrics, use evaluation set created earlier.\n",
    "predict_df_20 = pd.read_csv('data/metric_sample.csv', index_col=0)\n",
    "predict_df_20['predictions'] = 2.5 # fill in this value temporally\n",
    "eval_20 = Dataset.load_from_df(predict_df_20[['user_id','business_id','predictions']], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_before_20 = eval_20.build_full_trainset()\n",
    "eval_sr_20 = eval_before_20.build_testset()\n",
    "eval_pred_20 = algo_real.test(eval_sr_20)\n",
    "\n",
    "baseline_20 = pd.DataFrame(eval_pred_20, columns = ['userId','itemId','rating','pred_rating','x'])\n",
    "predict_df_20['predictions'] = baseline_20.pred_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11395348837209303 0.476046511627907 0.9672093023255807 541.146511627907\n"
     ]
    }
   ],
   "source": [
    "top_10, coverage, serendipity, avg_rank = get_all_metrics(predict_df_20, out, ratings_train_final)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
